#!/usr/bin/python
# Copyright (C) 2016 Linaro Limited
#
# Author: Remi Duraffort <remi.duraffort@linaro.org>
#
# This file is part of LAVA Dispatcher.
#
# LAVA Coordinator is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# LAVA Coordinator is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, see <http://www.gnu.org/licenses>.

import argparse
import logging
import os
import sys
import yaml

from lava_dispatcher.action import JobError
from lava_dispatcher.job import ZMQConfig
from lava_dispatcher.log import YAMLLogger
from lava_dispatcher.device import NewDevice
from lava_dispatcher.parser import JobParser


def run_pipeline_job(job, validate_only):
    # always validate every pipeline before attempting to run.
    exitcode = 0
    try:
        job.validate(simulate=validate_only)
        if not validate_only:
            exitcode = job.run()
    except (JobError, RuntimeError, TypeError, ValueError):
        import traceback
        traceback.print_exc()
        sys.exit(2)
    if exitcode:
        sys.exit(exitcode)


def is_pipeline_job(filename):
    # FIXME: use the schema once it is available
    if filename.lower().endswith('.yaml') or filename.lower().endswith('.yml'):
        return True
    return False


def parse_job_file(args, filename):
    """
    Uses the parsed device_config instead of the old Device class
    so it can fail before the Pipeline is made.
    Avoids loading all configuration for all supported devices for every job.
    """
    # Prepare the pipeline from the file using the parser.
    device = None  # secondary connections do not need a device
    if args.target:
        device = NewDevice(args.target)  # DeviceParser
    parser = JobParser()
    job = None

    # Load the configuration files (this should *not* fail)
    env_dut = None
    if args.env_dut_path is not None:
        with open(args.env_dut_path, 'r') as f_in:
            env_dut = f_in.read()
    dispatcher_config = None
    if args.dispatcher is not None:
        with open(args.dispatcher, "r") as f_in:
            dispatcher_config = f_in.read()

    try:
        # Create the ZMQ config
        zmq_config = ZMQConfig(args.socket_addr,
                               args.master_cert,
                               args.slave_cert)
        # Generate the pipeline
        with open(filename) as f_in:
            job = parser.parse(f_in, device, args.job_id,
                               zmq_config=zmq_config,
                               dispatcher_config=dispatcher_config,
                               output_dir=args.output_dir,
                               env_dut=env_dut)
        # Generate the description
        description = job.describe()
        description_file = os.path.join(args.output_dir,
                                        'description.yaml')
        if not os.path.exists(args.output_dir):
            os.makedirs(args.output_dir, 0o755)
        with open(description_file, 'w') as f_describe:
            f_describe.write(yaml.dump(description))

    except JobError as exc:
        logging.error("Invalid job submission: %s", exc)
        exit(1)
    # FIXME: NewDevice schema needs a validation parser
    # device.check_config(job)
    return job


def main():
    # Setup the parser
    # Don't put any default value as it has to be defined by the calling
    # process
    parser = argparse.ArgumentParser(description="lava dispatch")
    parser.add_argument("--output-dir", default=None,
                        help="Directory to put structured output in.")
    parser.add_argument("--validate", action='store_true',
                        help="Just validate the job file, do not execute any steps.")
    parser.add_argument("--job-id", action='store', default=None, required=True,
                        help=("Set the scheduler job identifier. "
                              "This alters process name for easier debugging"))
    parser.add_argument("--socket-addr", default=None,
                        help="Address of the ZMQ socket used to send the logs to the master")
    parser.add_argument("--master-cert", default=None,
                        help="Master certificate file")
    parser.add_argument("--slave-cert", default=None,
                        help="Slave certificate file")
    parser.add_argument("--target", default=None,
                        help="Run the job on a specific target device")
    parser.add_argument("--dispatcher", default=None,
                        help="The dispatcher configuration")
    parser.add_argument("--env-dut-path", default=None,
                        help="File with environment variables to be exported to the device")
    parser.add_argument("job_file", metavar="JOB", help="Test scenario file")
    args = parser.parse_args()

    # Check the requirements
    if os.geteuid() != 0:
        logging.error("You need to be root to run lava-dispatch.")
        exit(1)

    if not is_pipeline_job(args.job_file):
        logging.error("v1 jobs are not supported on this instance.")
        exit(1)

    # Pipeline always log as YAML so change the base logger.
    # Every calls to logging.getLogger will now return a YAMLLogger.
    logging.setLoggerClass(YAMLLogger)

    # Set process id based on the job-id
    try:
        from setproctitle import setproctitle
    except ImportError:
        logging.warning(
            ("Unable to import 'setproctitle', "
             "process name cannot be changed"))
    else:
        setproctitle("lava-dispatch [job: %s]" % args.job_id)

    # Load the job file
    job = parse_job_file(args, args.job_file)
    job_data = job.parameters

    if args.output_dir and not os.path.isdir(args.output_dir):
        os.makedirs(args.output_dir)

    # TODO: is it still usable?
    if args.target is None:
        if 'target' not in job_data:
            logging.error("The job file does not specify a target device. "
                          "You must specify one using the --target option.")
            exit(1)
    else:
        job_data['target'] = args.target

    run_pipeline_job(job, args.validate)


if __name__ == '__main__':
    main()
